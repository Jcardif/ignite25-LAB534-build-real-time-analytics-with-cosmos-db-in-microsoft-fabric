{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263b1193-87a0-4d0f-a4b3-42f8324e64b4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Reverse ETL and Personalization\n",
    "\n",
    "This notebook demonstrates a complete **Reverse ETL** pipeline that:\n",
    "1. **Extracts** aggregated loyalty data from the Data Warehouse\n",
    "2. **Transforms** and updates customer profiles in Cosmos DB\n",
    "3. **Builds personalized recommendations** using purchase history\n",
    "4. **Loads** enriched customer data back to Cosmos DB\n",
    "\n",
    "The process keeps operational data (Cosmos DB) synchronized with analytical insights from the warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0e313-5655-4383-8e20-523b53f803ea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "%pip install azure-cosmos\n",
    "%pip install azure-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075829f-d4d0-4df1-ba20-14ceed7df509",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - Update these endpoints for your environment\n",
    "import base64, json\n",
    "from typing import Any, Optional\n",
    "from azure.cosmos import CosmosClient, PartitionKey, ThroughputProperties\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "\n",
    "# ========== REQUIRED CONFIGURATION ==========\n",
    "# 1. Paste your Cosmos DB endpoint here (found in Settings > Connection)\n",
    "COSMOS_ENDPOINT = ''  # e.g., 'https://your-cosmos-account.documents.azure.com:443/'\n",
    "\n",
    "# 2. Paste your SQL Endpoint here (found in Warehouse > Settings > SQL Endpoint)  \n",
    "WAREHOUSE_SERVER = ''  # e.g., 'abcd1234-...-workspace.z01.datawarehouse.fabric.microsoft.com'\n",
    "\n",
    "# ========== DATABASE AND CONTAINER NAMES ==========\n",
    "COSMOS_DATABASE_NAME = 'fc_commerce_cosmos' \n",
    "COSMOS_CONTAINER_NAME = 'customers'\n",
    "WAREHOUSE_NAME = 'fc_commerce_warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff89755-6811-4815-a937-c37bcbeccd01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Authentication and Client Setup\n",
    "class FabricTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Cosmos DB access with automatic refresh and retry logic.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        access_token = notebookutils.credentials.getToken(\"https://cosmos.azure.com/.default\")\n",
    "        parts = access_token.split(\".\")\n",
    "        if len(parts) < 2:\n",
    "            raise ValueError(\"Invalid JWT format\")\n",
    "        payload_b64 = parts[1]\n",
    "        # Fix padding\n",
    "        padding = (-len(payload_b64)) % 4\n",
    "        if padding:\n",
    "            payload_b64 += \"=\" * padding\n",
    "        payload_json = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "        payload = json.loads(payload_json)\n",
    "        exp = payload.get(\"exp\")\n",
    "        if exp is None:\n",
    "            raise ValueError(\"exp claim missing in token\")\n",
    "        return AccessToken(token=access_token, expires_on=exp)\n",
    "\n",
    "# Initialize Cosmos DB clients\n",
    "COSMOS_CLIENT = CosmosClient(COSMOS_ENDPOINT, FabricTokenCredential())\n",
    "DATABASE_CLIENT = COSMOS_CLIENT.get_database_client(COSMOS_DATABASE_NAME)\n",
    "CONTAINER_CLIENT = DATABASE_CLIENT.get_container_client(COSMOS_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1142a2-d45a-426e-90d8-d617f2775b4b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Table Names and Utility Functions\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from notebookutils.mssparkutils import credentials\n",
    "\n",
    "# Data Warehouse table names\n",
    "DW_FACT_SALES = \"dbo.FactSales\"\n",
    "DW_FACT_SALES_LINE = \"dbo.FactSalesLineItems\"  \n",
    "DW_DIM_CUSTOMER = \"dbo.DimCustomer\"\n",
    "DW_DIM_MENU = \"dbo.DimMenuItem\"\n",
    "\n",
    "def get_warehouse_connection():\n",
    "    \"\"\"Get JDBC connection properties for the data warehouse.\"\"\"\n",
    "    token = credentials.getToken(\"pbi\")\n",
    "    jdbc_url = (\n",
    "        f\"jdbc:sqlserver://{WAREHOUSE_SERVER}:1433;\"\n",
    "        f\"database={WAREHOUSE_NAME};\"\n",
    "        \"encrypt=true;\"\n",
    "        \"trustServerCertificate=false;\"\n",
    "        \"hostNameInCertificate=*.datawarehouse.fabric.microsoft.com;\"\n",
    "        \"loginTimeout=30\"\n",
    "    )\n",
    "    props = {\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"accessToken\": token\n",
    "    }\n",
    "    return jdbc_url, props\n",
    "\n",
    "def read_dw_table(table_name: str):\n",
    "    \"\"\"Read a table from the data warehouse.\"\"\"\n",
    "    jdbc_url, props = get_warehouse_connection()\n",
    "    return spark.read.jdbc(jdbc_url, table=table_name, properties=props)\n",
    "\n",
    "def query_cosmos(query_text: str, parameters=None):\n",
    "    \"\"\"Execute a query against Cosmos DB and return results as Spark DataFrame.\"\"\"\n",
    "    if parameters is None:\n",
    "        parameters = []\n",
    "    \n",
    "    results = CONTAINER_CLIENT.query_items(\n",
    "        query=query_text,\n",
    "        parameters=parameters,\n",
    "        enable_cross_partition_query=True,\n",
    "    )\n",
    "    results_list = list(results)\n",
    "    print(f\"Retrieved {len(results_list)} records from Cosmos.\")\n",
    "    return spark.createDataFrame(results_list) if results_list else None\n",
    "\n",
    "def patch_cosmos_customer(customer_id: str, patch_operations: list):\n",
    "    \"\"\"Patch a customer document in Cosmos DB.\"\"\"\n",
    "    try:\n",
    "        CONTAINER_CLIENT.patch_item(\n",
    "            item=customer_id, \n",
    "            partition_key=customer_id, \n",
    "            patch_operations=patch_operations\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Patch failed for {customer_id}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff9e8e-ec1c-4c63-aabd-c290e5159777",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Test Query\n",
    "query_text = \"\"\"SELECT * FROM customers c\"\"\"\n",
    "\n",
    "df = query_cosmos(query_text)\n",
    "if df:\n",
    "    print(f\"Customer recommendation status: {df.count()} records\")\n",
    "    display(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d961d0",
   "metadata": {},
   "source": [
    "## Part 1: Reverse ETL - Loyalty Point Synchronization\n",
    "\n",
    "**Reverse ETL** moves processed data from analytical systems back to operational systems. Here we:\n",
    "- Extract aggregated loyalty points from the Data Warehouse\n",
    "- Calculate net loyalty balances (earned - redeemed)\n",
    "- Update customer profiles in Cosmos DB with current loyalty totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b490b-f147-4430-a3bf-ff3f39b19805",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Load warehouse data and calculate loyalty point updates\n",
    "print(\"Loading data warehouse tables...\")\n",
    "fact_sales=read_dw_table(DW_FACT_SALES)\n",
    "fact_line  = read_dw_table(DW_FACT_SALES_LINE).select(\"TransactionId\",\"MenuItemKey\",\"Quantity\")\n",
    "dim_menu= read_dw_table(DW_DIM_MENU).select(\"MenuItemKey\",\"MenuItemId\",\"MenuItemName\")\n",
    "dim_customer = read_dw_table(DW_DIM_CUSTOMER).select(\"CustomerKey\",\"CustomerId\")\n",
    "\n",
    "print(\"Tables loaded successfully!\")\n",
    "print(f\"Fact Sales: {fact_sales.count()} records\")\n",
    "print(f\"Fact Line Items: {fact_line_items.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00740c1-7fde-4a63-ae27-2a63cb12f959",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate loyalty deltas from FactSales\n",
    "loyalty_agg = (fact_sales\n",
    "    .groupBy(\"CustomerKey\")\n",
    "    .agg(\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsEarned\"), F.lit(0)).alias(\"PointsEarned\"),\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsRedeemed\"), F.lit(0)).alias(\"PointsRedeemed\"),\n",
    "        F.max(\"CreatedAt\").alias(\"LastPurchaseUtc\")\n",
    "    )\n",
    "    .withColumn(\"NewLoyaltyPoints\", F.col(\"PointsEarned\") - F.col(\"PointsRedeemed\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdc436-47c8-4777-975a-02ae8362c0bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Load all customers from Cosmos DB\n",
    "customers_df = query_cosmos(\"SELECT * FROM c\")\n",
    "if customers_df:\n",
    "    print(f\"Loaded {customers_df.count()} customers from Cosmos DB\")\n",
    "    customers_df.select(\"customerId\", \"name\", \"loyaltyPoints\", \"lastPurchaseDate\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d2284-f18a-4743-9f16-8cd30ad56ee3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Bring CustomerId to join with Cosmos (via DimCustomer)\n",
    "dim_customer = read_dw_table(DW_DIM_CUSTOMER).select(\"CustomerKey\",\"CustomerId\")\n",
    "\n",
    "loyalty_by_id = (loyalty_agg\n",
    "    .join(dim_customer, on=\"CustomerKey\", how=\"inner\")\n",
    "    .select(\"CustomerId\",\"NewLoyaltyPoints\",\"LastPurchaseUtc\")\n",
    ")\n",
    "\n",
    "# Join with existing customer docs and produce upserts\n",
    "# Keep all fields in the doc, only replace loyaltyPoints / lastPurchaseDate / updatedAt\n",
    "cust_with_loyalty = (customers_df.alias(\"c\")\n",
    "    .join(loyalty_by_id.alias(\"l\"), F.col(\"c.customerId\") == F.col(\"l.CustomerId\"), \"left\")\n",
    "    .withColumn(\"loyaltyPoints\", F.when(F.col(\"l.NewLoyaltyPoints\").isNotNull(), F.col(\"l.NewLoyaltyPoints\")).otherwise(F.col(\"c.loyaltyPoints\")))\n",
    "    .withColumn(\"lastPurchaseDate\", F.when(F.col(\"l.LastPurchaseUtc\").isNotNull(), F.col(\"l.LastPurchaseUtc\").cast(\"timestamp\")).otherwise(F.col(\"c.lastPurchaseDate\")))\n",
    "    .withColumn(\"updatedAt\", F.current_timestamp())\n",
    "    .drop(\"NewLoyaltyPoints\",\"LastPurchaseUtc\",\"CustomerKey\")\n",
    ")\n",
    "\n",
    "display(cust_with_loyalty.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252d145-4987-4279-852d-aad15a0416fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate loyalty point updates from warehouse data\n",
    "loyalty_agg = (fact_sales\n",
    "    .groupBy(\"CustomerKey\")\n",
    "    .agg(\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsEarned\"), F.lit(0)).alias(\"PointsEarned\"),\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsRedeemed\"), F.lit(0)).alias(\"PointsRedeemed\"),\n",
    "        F.max(\"CreatedAt\").alias(\"LastPurchaseUtc\")\n",
    "    )\n",
    "    .withColumn(\"NewLoyaltyPoints\", F.col(\"PointsEarned\") - F.col(\"PointsRedeemed\"))\n",
    ")\n",
    "\n",
    "# Join with customer dimension to get CustomerId\n",
    "loyalty_by_id = (loyalty_agg\n",
    "    .join(dim_customer.select(\"CustomerKey\", \"CustomerId\"), on=\"CustomerKey\", how=\"inner\")\n",
    "    .select(\"CustomerId\", \"NewLoyaltyPoints\", \"LastPurchaseUtc\")\n",
    ")\n",
    "\n",
    "# Filter to customers with purchases today\n",
    "today_updates = (loyalty_by_id\n",
    "    .filter(F.to_date(\"LastPurchaseUtc\") == F.current_date())\n",
    "    .select(\"CustomerId\", \"NewLoyaltyPoints\", \"LastPurchaseUtc\")\n",
    ")\n",
    "\n",
    "print(f\"Customers with purchases today: {today_updates.count()}\")\n",
    "today_updates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b40d2-a4d8-403e-9db3-ee0e8433f7f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Update customer loyalty points in Cosmos DB\n",
    "patched, failed = 0, 0\n",
    "\n",
    "for row in today_updates.toLocalIterator():\n",
    "    cust_id = row[\"CustomerId\"]\n",
    "    points = int(row[\"NewLoyaltyPoints\"] or 0)\n",
    "    last_dt = row[\"LastPurchaseUtc\"]\n",
    "    last_iso = last_dt.isoformat().replace(\"+00:00\", \"Z\") if last_dt.tzinfo else last_dt.isoformat() + \"Z\"\n",
    "\n",
    "    ops = [\n",
    "        {\"op\": \"set\", \"path\": \"/loyaltyPoints\", \"value\": points},\n",
    "        {\"op\": \"set\", \"path\": \"/lastPurchaseDate\", \"value\": last_iso},\n",
    "        {\"op\": \"set\", \"path\": \"/updatedAt\", \"value\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "    ]\n",
    "\n",
    "    if patch_cosmos_customer(cust_id, ops):\n",
    "        patched += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"✅ Patched: {patched} | ❌ Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018892e-7ae2-462c-81ea-e11ad0ced2a3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Verify loyalty point updates\n",
    "check_query = \"\"\"\n",
    "SELECT TOP 10 c.customerId, c.name, c.loyaltyPoints, c.lastPurchaseDate, c.updatedAt\n",
    "FROM c\n",
    "WHERE IS_DEFINED(c.updatedAt)\n",
    "ORDER BY c.updatedAt DESC\n",
    "\"\"\"\n",
    "\n",
    "df_check = query_cosmos(check_query)\n",
    "if df_check:\n",
    "    print(f\"Retrieved {df_check.count()} recently updated customers\")\n",
    "    display(df_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b676a97",
   "metadata": {},
   "source": [
    "## Part 2: Personalized Recommendations\n",
    "### Step 1: Calculate Weighted Purchase Scores & Build Top Recommendations\n",
    "- Join transaction data, apply exponential decay for recency, and normalize scores (0-1) per customer.\n",
    "- Min-max normalization to select top 4 items with scores ≥ 0.70 per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb8e65-aa9f-44cc-a434-69135f74d03c",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Personalized Recommendations\n",
    "from pyspark.sql import Window\n",
    "from math import log\n",
    "\n",
    "# Recommendation algorithm parameters\n",
    "TOPK = 4\n",
    "MIN_SCORE = 0.70\n",
    "HALF_LIFE_DAYS = 14  # Recency decay (set to 0 to disable)\n",
    "LAMBDA = (log(2) / HALF_LIFE_DAYS) if HALF_LIFE_DAYS > 0 else 0.0\n",
    "\n",
    "def calculate_recommendations():\n",
    "    \"\"\"Calculate personalized recommendations based on purchase history.\"\"\"\n",
    "    \n",
    "    # Get transaction data with customer and menu item details\n",
    "    lines = (fact_line\n",
    "             .join(fact_sales, on=\"TransactionId\", how=\"inner\")\n",
    "             .select(\"CustomerKey\", \"MenuItemKey\", \"Quantity\", \"CreatedAt\"))\n",
    "    \n",
    "    # Apply recency decay if enabled\n",
    "    if LAMBDA > 0:\n",
    "        lines = (lines\n",
    "                .withColumn(\"ageDays\", F.datediff(F.current_timestamp(), F.col(\"CreatedAt\")))\n",
    "                .withColumn(\"decay\", F.exp(-F.lit(LAMBDA) * F.col(\"ageDays\")))\n",
    "                .withColumn(\"wQty\", F.col(\"Quantity\") * F.col(\"decay\")))\n",
    "    else:\n",
    "        lines = lines.withColumn(\"wQty\", F.col(\"Quantity\"))\n",
    "    \n",
    "    # Aggregate weighted quantity per (customer, item)\n",
    "    cust_item = (lines\n",
    "                .groupBy(\"CustomerKey\", \"MenuItemKey\")\n",
    "                .agg(F.sum(\"wQty\").alias(\"wQty\")))\n",
    "    \n",
    "    print(f\"Unique (Customer, Item) pairs: {cust_item.count()}\")\n",
    "    \n",
    "    # Normalize scores 0..1 per customer\n",
    "    win_c = Window.partitionBy(\"CustomerKey\")\n",
    "    scored = (cust_item\n",
    "             .withColumn(\"minQ\", F.min(\"wQty\").over(win_c))\n",
    "             .withColumn(\"maxQ\", F.max(\"wQty\").over(win_c))\n",
    "             .withColumn(\"score\",\n",
    "                        F.when(F.col(\"maxQ\") == F.col(\"minQ\"), F.lit(1.0))\n",
    "                         .otherwise((F.col(\"wQty\") - F.col(\"minQ\")) / (F.col(\"maxQ\") - F.col(\"minQ\"))))\n",
    "             .select(\"CustomerKey\", \"MenuItemKey\", F.round(\"score\", 4).alias(\"score\")))\n",
    "    \n",
    "    return scored\n",
    "\n",
    "# Calculate recommendation scores\n",
    "recommendation_scores = calculate_recommendations()\n",
    "print(\"Recommendation scores calculated successfully!\")\n",
    "recommendation_scores.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db75c10",
   "metadata": {},
   "source": [
    "### Step 2: Join with Dimension Tables & Create Recommendation Payload\n",
    "- Join with product and customer dimension tables to enrich the recommendation data.\n",
    "- Create a structured payload for the recommendation engine, including customer ID, product ID, and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3a05c-566d-4cef-8a89-8f70445195e8",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare top recommendations for each customer\n",
    "def prepare_recommendations(scored_df):\n",
    "    \"\"\"Prepare top recommendations with menu item details.\"\"\"\n",
    "    \n",
    "    # Join with customer and menu dimensions\n",
    "    scored_with_details = (scored_df\n",
    "                          .join(dim_customer.select(\"CustomerKey\", \"CustomerId\"), \n",
    "                                on=\"CustomerKey\", how=\"inner\")\n",
    "                          .join(dim_menu.select(\"MenuItemKey\", \"MenuItemId\", \"MenuItemName\"), \n",
    "                                on=\"MenuItemKey\", how=\"left\")\n",
    "                          .select(\"CustomerId\", \"MenuItemId\", \"MenuItemName\", \"score\"))\n",
    "    \n",
    "    # Filter and rank per customer\n",
    "    win_rank = Window.partitionBy(\"CustomerId\").orderBy(F.col(\"score\").desc())\n",
    "    top_recs = (scored_with_details\n",
    "               .filter(F.col(\"score\") >= F.lit(MIN_SCORE))\n",
    "               .withColumn(\"rnk\", F.row_number().over(win_rank))\n",
    "               .filter(F.col(\"rnk\") <= TOPK)\n",
    "               .drop(\"rnk\"))\n",
    "    \n",
    "    print(f\"Customers with ≥{MIN_SCORE} score items: {top_recs.select('CustomerId').distinct().count()}\")\n",
    "    \n",
    "    # Build recommendations array payload\n",
    "    recs_ready = (top_recs\n",
    "                 .withColumn(\"rec_item\", F.struct(\n",
    "                     F.col(\"MenuItemId\").alias(\"menuItemId\"),\n",
    "                     F.col(\"MenuItemName\").alias(\"name\"),\n",
    "                     F.col(\"score\").alias(\"score\"),\n",
    "                     F.lit(\"High-frequency purchase\").alias(\"reason\")))\n",
    "                 .groupBy(\"CustomerId\")\n",
    "                 .agg(F.collect_list(\"rec_item\").alias(\"recommendations\")))\n",
    "    \n",
    "    print(f\"Total customers with recommendations: {recs_ready.count()}\")\n",
    "    return recs_ready\n",
    "\n",
    "# Prepare final recommendations\n",
    "final_recommendations = prepare_recommendations(recommendation_scores)\n",
    "display(final_recommendations.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845aff33",
   "metadata": {},
   "source": [
    "### Step 3: Update Customer Profiles\n",
    "Patch Cosmos DB documents with recommendation arrays and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459870e9-c402-4167-be9c-ebcd5077a63f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Update customer recommendations in Cosmos DB\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "def update_customer_recommendations(recommendations_df):\n",
    "    \"\"\"Update customer documents with personalized recommendations.\"\"\"\n",
    "    patched, failed = 0, 0\n",
    "    \n",
    "    for row in recommendations_df.toLocalIterator():\n",
    "        cust_id = row[\"CustomerId\"]\n",
    "        recs = [x.asDict(recursive=True) if isinstance(x, Row) else x for x in row[\"recommendations\"]]\n",
    "        \n",
    "        ops = [\n",
    "            {\"op\": \"set\", \"path\": \"/recommendations\", \"value\": recs},\n",
    "            {\"op\": \"set\", \"path\": \"/updatedAt\", \"value\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "        ]\n",
    "        \n",
    "        if patch_cosmos_customer(cust_id, ops):\n",
    "            patched += 1\n",
    "            if patched % 100 == 0:\n",
    "                print(f\"Patched {patched} customers so far...\")\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"✅ Patched {patched} customers, ❌ Failed {failed}\")\n",
    "    return patched, failed\n",
    "\n",
    "# Update recommendations in Cosmos DB\n",
    "update_customer_recommendations(final_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a4c39-ea8e-4a24-84c9-49b5bced91ac",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Verify recommendations were successfully added\n",
    "check_query = \"\"\"\n",
    "SELECT TOP 10 c.customerId, c.loyaltyPoints, c.recommendations, c.updatedAt\n",
    "FROM c\n",
    "WHERE IS_DEFINED(c.recommendations) AND ARRAY_LENGTH(c.recommendations) > 0\n",
    "ORDER BY c.updatedAt DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = query_cosmos(check_query)\n",
    "if result_df:\n",
    "    print(f\"Found {result_df.count()} customers with recommendations.\")\n",
    "    # Convert to pandas for better display of nested data\n",
    "    result_pandas = result_df.toPandas()\n",
    "    display(result_pandas)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
